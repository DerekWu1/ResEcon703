\documentclass{beamer}
\usetheme{Boadilla}

\makeatother
\setbeamertemplate{footline}
{
    \leavevmode%
    \hbox{%
    \begin{beamercolorbox}[wd=.4\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
        \usebeamerfont{author in head/foot}\insertshortauthor
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.55\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
        \usebeamerfont{title in head/foot}\insertshorttitle
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.05\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
        \insertframenumber{}
    \end{beamercolorbox}}%
    \vskip0pt%
}
\makeatletter
\setbeamertemplate{navigation symbols}{}

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\renewcommand{\familydefault}{\sfdefault}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\plim}{plim}

\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{threeparttable}
\usepackage{booktabs}
\usepackage{siunitx}
\sisetup{parse-numbers=false}

%\setlength{\OuterFrameSep}{-2pt}
%\makeatletter
%\preto{\@verbatim}{\topsep=-10pt \partopsep=-10pt }
%\makeatother

\title[Lecture 9:\ Nonlinear Least Squares I]{Lecture 9:\ Nonlinear Least Squares I}
\author[ResEcon 703:\ Advanced Econometrics]{ResEcon 703:\ Topics in Advanced Econometrics}
\date{Matt Woerman\\University of Massachusetts Amherst}

\begin{document}

{\setbeamertemplate{footline}{} 
\begin{frame}[noframenumbering]
    \titlepage
\end{frame}
}

\begin{frame}\frametitle{Agenda}
    Last time
    \begin{itemize}
        \item Logit Estimation Using MLE
    \end{itemize}
    \vspace{2ex}
    Today
    \begin{itemize}
    	\item Nonlinear Least Squares
    	\item Properties of the Nonlinear Least Squares Estimator
    	\item Computing the Nonlinear Least Squares Estimator
    \end{itemize}
    \vspace{2ex}
    Upcoming
    \begin{itemize}
        \item Reading for next time
        \begin{itemize}
            \item Optional: Schaefer (1998)
        \end{itemize}
        \item Problem sets
        \begin{itemize}
            \item Problem Set 2 is posted, due October 17
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Maximum Likelihood Recap}
    If we are willing to make a strong assumption about the density of our data 
    $$f(y \mid \beta)$$
    then the MLE gives us the ``best'' consistent estimator of $\beta$
    \begin{itemize}
    	\item ``Best'' means the estimator with the lowest variance
    \end{itemize}
    \vspace{3ex}
    But what if we are not willing to make such a strong assumption about our data?
    \begin{itemize}
    	\item Note: For the logit model, we are already making some strong assumptions, so the MLE assumption is not unreasonable. But in other settings, this assumption may be overly restrictive.
    \end{itemize}
 \end{frame}

\begin{frame}\frametitle{}
    \vfill
    \centering
    \begin{beamercolorbox}[center]{title}
        \Large Nonlinear Least Squares
    \end{beamercolorbox}
    \vfill
\end{frame}

\begin{frame}\frametitle{Nonlinear Regression Models}
    The general formula for a nonlinear regression model
    $$y_i = h(x_i, \beta) + \varepsilon_i$$
    \begin{itemize}
        \item OLS (linear model) is a special case
        \item Some models that appear to be nonlinear can be simplified to a linear model
        \begin{itemize}
            \item See the Cobb-Douglas production function example from Lecture 1
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Examples of Nonlinear Regression Models}
    Binary discrete choice
    \begin{align*}
        u_i &= \beta' x_i + \varepsilon_i \\
        y_i &= 
            \begin{cases}
                1 & \text{ if } u_i > 0 \\
                0 & \text{ otherwise}
            \end{cases}
    \end{align*}
    \vspace{1ex}
    CES production function
    $$\ln y_i = \ln \gamma - \frac{\upsilon}{\rho} \ln \left[ \delta K_i^{-\rho} + \left( 1 - \delta \right) L_i^{-\rho} \right] + \varepsilon_i$$
    \vspace{1ex}
    Exponential regression
    $$y_i = \beta_0 + \beta_1 e^{\beta _2 x_{i1} + \beta_3 x_{i2}} + \varepsilon _i$$
\end{frame}

\begin{frame}\frametitle{Nonlinear Least Squares Assumption}
    \textbf{NLS Assumption} \\
    \vspace{3ex}
    For the nonlinear regression model $y_i = h(x_i, \beta) + \varepsilon_i$, we assume that
    \begin{enumerate}
    	\item $\frac{1}{n} \sum_{i = 1}^n \left( \frac{\partial h(x_i, \beta_0)}{\partial \beta_0} \right) \left( \frac{\partial h(x_i, \beta_0)}{\partial \beta_0'} \right) \overset{p}{\rightarrow} Q^0$
    	\item $\frac{1}{n} \sum_{i = 1}^n \frac{\partial h(x_i, \beta_0)}{\partial \beta_0} \varepsilon_i \overset{p}{\rightarrow} 0$
    	\item $\frac{1}{\sqrt{n}} \sum_{i = 1}^n \frac{\partial h(x_i, \beta_0)}{\partial \beta_0} \varepsilon_i \overset{d}{\rightarrow} N(0, \sigma^2 Q^0)$
    \end{enumerate}
    where $Q^0$ is a positive definite matrix and $\sigma^2$ is a finite constant \\
    \vspace{3ex}
    What is a further assumption that almost guarantees this to be true?
    \begin{itemize}
    	\item Strict independence of $\varepsilon_i$ and $x_i$: $f(\varepsilon_i) = f(\varepsilon_i \mid x_i)$
    	\item $\varepsilon_i$ is normally distributed: $\varepsilon_i \sim N(0, \sigma^2)$
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Nonlinear Least Squares Estimator}
    Just as with ordinary least squares (OLS), we seek to minimize the sum of squared errors
    $$S(\beta) = \sum_{i = 1}^n [y_i - h(x_i, \beta)]^2$$
    The nonlinear least squares estimator is defined as
    $$\hat{\beta} = \argmin_{\beta} S(\beta)$$
    The first-order condition for this minimization is
    $$\sum_{i = 1}^n [y_i - h(x_i, \beta)] \frac{\partial h(x_i, \beta)}{\partial \beta} = 0$$ \\
    \begin{itemize}
    	\item Unlike OLS, this estimator may not have a closed-form expression
    	\item This is a moment condition that we could use for GMM
    	\begin{itemize}
    		\item More on that next week...
    	\end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Nonlinear Least Squares Example}
	We want to estimate the parameters of
    $$y_i = \beta_1 + \beta_2 e^{\beta_3 x_i} + \varepsilon_i$$
    Nonlinear least squares will find the set of parameters that minimizes
    $$S(\beta) = \sum_{i = 1}^n [y_i - (\beta_1 + \beta_2 e^{\beta_3 x_i})]^2$$
    The first-order conditions are
    \begin{align*}
    	\frac{\partial S(\beta)}{\partial \beta_1} &= -\sum_{i = 1}^n [y_i - (\beta_1 + \beta_2 e^{\beta_3 x_i})] = 0 \\
    	\frac{\partial S(\beta)}{\partial \beta_2} &= -\sum_{i = 1}^n [y_i - (\beta_1 + \beta_2 e^{\beta_3 x_i})] e^{\beta_3 x_i} = 0 \\
    	\frac{\partial S(\beta)}{\partial \beta_3} &= -\sum_{i = 1}^n [y_i - (\beta_1 + \beta_2 e^{\beta_3 x_i})] \beta_2 x_i e^{\beta_3 x_i} = 0
    \end{align*}
\end{frame}

\begin{frame}\frametitle{Nonlinear Least Squares for the Logit Model}
    We can think of regressing a choice on logit choice probabilities
    \begin{align*}
    	y_{ni} &= P_{ni} + \omega_{ni} \\
    	\intertext{If we substitute in $P_{ni}$ and assume linear representative utility, we get}
    	y_{ni} &= \frac{e^{\beta' x_{ni}}}{\sum_{j = 1}^J e^{\beta' x_{nj}}} + \omega_{ni}
    \end{align*}
    We can use nonlinear least squares to find the set of parameters, $\hat{\beta}$, that minimizes
    $$S(\beta) = \sum_{n = 1}^N \sum_{i = 1}^J \left[ y_{ni} - \frac{e^{\beta' x_{ni}}}{\sum_{j = 1}^J e^{\beta' x_{nj}}} \right]^2$$ \\
    \vspace{2ex}
    How is this different from our MLE logit estimation?
\end{frame}

\begin{frame}\frametitle{}
    \vfill
    \centering
    \begin{beamercolorbox}[center]{title}
        \Large Properties of the Nonlinear Least Squares Estimator
    \end{beamercolorbox}
    \vfill
\end{frame}

\begin{frame}\frametitle{Consistency of the NLS Estimator}
    If the following assumptions hold
    \begin{enumerate}
    	\item The parameter space containing $\beta_0$ is compact (no gaps or non-concave regions)
    	\item For any vector $\beta$ in that parameter space $(1 / n) S(\beta) \overset{p}{\rightarrow} q(\beta)$, a continuous and differentiable function
    	\item $q(\beta)$ has a unique minimum at the true parameter vector, $\beta_0$
    \end{enumerate}
    then the nonlinear least squares estimator is consistent
    $$\hat{\beta} \overset{p}{\rightarrow} \beta_0$$
    where $\hat{\beta}$ is the NLS estimator and $\beta_0$ is the true parameter value(s)
\end{frame}

\begin{frame}\frametitle{Asymptotic Normality of the NLS Estimator}
    The asymptotic distribution of the NLS estimator, $\hat{\beta}$, is normal with mean at the true parameter value(s), $\beta_0$, and known variance
    $$\hat{\beta} \overset{a}{\sim} N \left[ \beta_0, \frac{\sigma^2}{n} (Q^0)^{-1} \right]$$
    where
    $$Q^0 = \plim \frac{1}{n} \sum_{i = 1}^n \left( \frac{\partial h(x_i, \beta_0)}{\partial \beta_0} \right) \left( \frac{\partial h(x_i, \beta_0)}{\partial \beta_0'} \right)$$ 
\end{frame}

\begin{frame}\frametitle{NLS Variance Estimator}
    From the asymptotic normality of the NLS estimator, we have
    $$Var(\hat{\beta}) = \frac{\sigma^2}{n} \left[ \plim \frac{1}{n} \sum_{i = 1}^n \left( \frac{\partial h(x_i, \beta_0)}{\partial \beta_0} \right) \left( \frac{\partial h(x_i, \beta_0)}{\partial \beta_0'} \right) \right]^{-1}$$ \\
    \begin{itemize}
        \item Variance of NLS estimator depends on $\beta_0$ and $\sigma^2$, which we do not know, so we need an estimator for the variance of the NLS estimator
    \end{itemize}
    \vspace{1ex}
    We can estimate this variance using estimators, $\hat{\beta}$ and $\hat{\sigma}^2$
    \begin{align*}
    	\widehat{Var}(\hat{\beta}) &= \hat{\sigma}^2 \left[ \sum_{i = 1}^n \left( \frac{\partial h(x_i, \beta)}{\partial \beta} \right)_{\hat{\beta}} \left( \frac{\partial h(x_i, \beta)}{\partial \beta'} \right)_{\hat{\beta}} \right]^{-1} \\
    	\intertext{where}
    	\hat{\sigma}^2 &= \frac{1}{n} \sum_{i = 1}^n [y_i - h(x_i, \hat{\beta})]^2
    \end{align*} \\
    \begin{itemize}
    	\item More robust estimators exist, but we will stick with this most basic variance estimator for now
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Estimating the NLS Variance-Covariance Matrix}
    $$\widehat{Var}(\hat{\beta}) = \hat{\sigma}^2 \left[ \sum_{i = 1}^n \left( \frac{\partial h(x_i, \beta)}{\partial \beta} \right)_{\hat{\beta}} \left( \frac{\partial h(x_i, \beta)}{\partial \beta'} \right)_{\hat{\beta}} \right]^{-1}$$
    Steps to estimate this variance-covariance matrix
    \begin{enumerate}
		\item Write down the derivative of the nonlinear regression model with respect to each of the $K$ parameters.
		\item Calculate this $K \times 1$ vector of derivatives, at the estimated parameters, for each decision maker.
		\item Calculate the $K \times K$ matrix that is the product of the above vector and its transpose for each decision maker.
		\item Sum these matrices for all decision makers.
		\item Estimate the variance of the econometric error as the mean sum of squares at the estimated parameters.
		\item Calculate the variance-covariance matrix, which is a function of the above $K \times K$ matrix and the estimated error variance.
	\end{enumerate}
\end{frame}

\begin{frame}\frametitle{NLS Goodness of Fit}
    There is no universally agreed upon measure to summarize the goodness of fit of a nonlinear least squares model \\
    \vspace{3ex}
    Some people calculate the traditional $R^2$
    $$R^2 = 1 - \frac{\sum_{i = 1}^n [y_i - h(x_i, \hat{\beta})]^2}{\sum_{i = 1}^n (y_i - \bar{y})^2}$$
    \begin{itemize}
    	\item But $R^2$ for a nonlinear model may not have a range of $[0, 1]$
    \end{itemize}
    \vspace{3ex}
    Many alternatives exist, and some may be useful depending on what you want your $R^2$ to summarize
\end{frame}

\begin{frame}\frametitle{Wald Test}
    A Wald test is used to test hypotheses about parameters fit by NLS
    $$H_0 \text{: } r(\beta_0) = q$$
    where $r$ is a vector of $J$ functions of parameters \\
    \vspace{2ex}
    The Wald test statistic is
    \begin{align*}
    	W &= [r(\hat{\beta}) - q]' [ R(\hat{\beta}) \widehat{Var}(\hat{\beta}) R'(\hat{\beta})]^{-1} [r(\hat{\beta}) - q] \\
    	\intertext{where $R$ is the $J \times K$ Jacobian}
    	R(\hat{\beta}) &= \left( \frac{\partial r(\beta)}{\partial \beta'} \right)_{\hat{\beta}}
    \end{align*} \\
    \vspace{2ex}
    This Wald test statistic converges in distribution to a chi-squared
    $$W \overset{d}{\rightarrow} \chi^2(J)$$
\end{frame}

\begin{frame}\frametitle{Conducting a NLS Wald Test}
	\begin{alignat*}{2}
		&\text{Hypothesis:} &&H_0 \text{: } r(\beta_0) = q \\
		&\text{Test statistic:} &&W = [r(\hat{\beta}) - q]' [ R(\hat{\beta}) \widehat{Var}(\hat{\beta}) R'(\hat{\beta})]^{-1} [r(\hat{\beta}) - q] \\
    	&\text{Jacobian matrix:} \quad &&R(\hat{\beta}) = \left( \frac{\partial r(\beta)}{\partial \beta'} \right)_{\hat{\beta}}
	\end{alignat*} \\
	\vspace{2ex}
    Steps to conduct a Wald Test using NLS estimators
    \begin{enumerate}
		\item Create a vector of $J$ parameter restrictions, $r(\beta_0) = q$.
		\item Calculate the $J \times K$ Jacobian matrix by differentiating each restriction with respect to each of the $K$ parameters.
		\item Calculate the Wald test statistic, which is a function of the vector of restrictions, the Jacobian matrix, and the variance-covariance matrix.
		\item Conduct the Wald test using this test statistic, which is distributed $\chi^2$.
	\end{enumerate}
\end{frame}

\begin{frame}\frametitle{}
    \vfill
    \centering
    \begin{beamercolorbox}[center]{title}
        \Large Computing the Nonlinear Least Squares Estimator
    \end{beamercolorbox}
    \vfill
\end{frame}

\begin{frame}\frametitle{Computing the Nonlinear Least Squares Estimator}
    One option for minimizing the sum of squared errors in a nonlinear regression is to use the numerical optimization techniques we talked about previously \\
    \vspace{3ex}
    An alternative option is to take advantage of the fact that we already know how to minimize sum of squared errors for a linear model using OLS, so if we can sufficiently approximate our nonlinear regression as a linear regression, we can use OLS to help us find the NLS estimator
    \begin{enumerate}
    	\item Begin with some initial parameter values, $\beta_1$
    	\item Approximate the regression model as a linear model given a set of parameters, $\beta_t$
    	\item Use OLS to find the next iteration of parameters, $\beta_{t + 1}$
    	\item Repeat (2) and (3) until the parameters converge
    \end{enumerate}
\end{frame}

\begin{frame}\frametitle{Linear Approximation of a Nonlinear Regression Model}
    A common method for minimizing the sum of squares in a NLS regression is the Gauss-Newton method, which approximates the nonlinear regression model, $h(x_i, \beta_{t + 1})$, as a linear Taylor approximation given a vector of parameters, $\beta_t$
    $$h(x_i, \beta_{t + 1}) \approx h(x_i, \beta_t) + (\beta_{t + 1} - \beta_t)' \left( \frac{\partial h(x_i, \beta_t)}{\partial \beta_t} \right)$$
    which implies that
    $$y_i - h(x_i, \beta_t) + \beta_t' \left( \frac{\partial h(x_i, \beta_t)}{\partial \beta_t} \right) = \beta_{t + 1}' \left( \frac{\partial h(x_i, \beta_t)}{\partial \beta_t} \right) + \varepsilon_{it}$$
    where $\varepsilon_{it}$ is an error term that includes the original econometric error, $\varepsilon_i$, and error due to the linear Taylor approximation
\end{frame}

\begin{frame}\frametitle{Iterate Parameters Using OLS}
    In that final linear equation
    $$y_i - h(x_i, \beta_t) + \beta_t' \left( \frac{\partial h(x_i, \beta_t)}{\partial \beta_t} \right) = \beta_{t + 1}' \left( \frac{\partial h(x_i, \beta_t)}{\partial \beta_t} \right) + \varepsilon_{it}$$
    the only unknowns are the next vector of parameters, $\beta_{t + 1}$, and the composite error term, $\varepsilon_{it}$
    \begin{itemize}
    	\item We start from a given vector of parameters, $\beta_t$, so $h(x_i, \beta_t)$ and $\partial h(x_i, \beta_t) / \partial \beta_t$ can be calculated or numerically approximated
    	\item We can think of this as a linear regression equation
    \end{itemize}
    \vspace{2ex}
    To get the next set of parameters, $\beta_{t + 1}$
    \begin{enumerate}
    	\item Calculate the left-hand side of this regression equation
    	\item Regress those values on $\partial h(x_i, \beta_t) / \partial \beta_t$
    \end{enumerate}
    \vspace{2ex}
    $\beta_{t + 1}$ is the result of this OLS regression
\end{frame}

\begin{frame}\frametitle{Iteration and Convergence}
    We have a closed-form solution for the parameters that result from an OLS regression, so we can more succinctly write
    \begin{align*}
    	\beta_{t + 1} = \beta_t + &\left\{ \sum_{i = 1}^n \left( \frac{\partial h(x_i, \beta)}{\partial \beta} \right)_{\beta_t} \left( \frac{\partial h(x_i, \beta)}{\partial \beta'} \right)_{\beta_t} \right\}^{-1} \\
    	& \qquad \times \left\{ \sum_{i = 1}^n \left( \frac{\partial h(x_i, \beta)}{\partial \beta} \right)_{\beta_t} [y_i - h(x_i, \beta_t)] \right\}
    \end{align*} \\
    \vspace{2ex}
    We use this formula to iterate through parameters until they converge 
    \begin{itemize}
    	\item See notes on numerical optimization for convergence criteria
    \end{itemize}
    \vspace{2ex}
    Alternatively, we can use the other numerical optimization methods we discussed previously
\end{frame}

\begin{frame}\frametitle{Announcements}
    Reading for next time
    \begin{itemize}
        \item Optional: Schaefer (1998)
    \end{itemize}
    \vspace{3ex}
    Upcoming
    \begin{itemize}
        \item Problem Set 2 is posted, due October 17
    \end{itemize}
\end{frame}

\end{document}